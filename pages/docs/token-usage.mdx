---
description: Langfuse tracks token usage for each generation for cost and usage reporting. If tokens are not provided in the generation, Langfuse will automatically calculate the token usage.
---

# Token usage

Token usage numbers are used across the Langfuse interface and reports.

```typescript
generation = {
  ...
  usage: {
    prompt: number,
    completion: number,
    total: number,
    unit: string, // 'TOKENS' or 'CHARACTERS', defaults to 'TOKENS'
  },
  ...
}
```

## Ingestion of usage

When ingesting LLM generations into Langfuse you can add token usage numbers to the generation object. If available in the LLM response, this is the preferred way to track usage with Langfuse.

## Built-in token calculation

For ingested generations without usage attributes, Langfuse automatically calculates token amounts. The correct tokenizer is selected based on the `model` attribute of the generation.

This is helpful for LLM-APIs that do not return usage information in the response, e.g., when streaming OpenAI completions.

| Model           | Tokenizer     | Package                                                                            |
| --------------- | ------------- | ---------------------------------------------------------------------------------- |
| `gpt*`          | `cl100k_base` | [`tiktoken`](https://www.npmjs.com/package/tiktoken)                               |
| `text-davinci*` | `p50k_base`   | [`tiktoken`](https://www.npmjs.com/package/tiktoken)                               |
| `claude*`       | `claude`      | [`@anthropic-ai/tokenizer`](https://www.npmjs.com/package/@anthropic-ai/tokenizer) |

Need another tokenizer? Create an issue on [GitHub](/issue).

## Compatibility with OpenAI

Langfuse is compatible with the usage objects returned by the OpenAI API. The Langfuse SDKs automatically convert the OpenAI usage object to the more generic Langfuse usage object.

```typescript
usage = {
  promptTokens: 50,
  completionTokens: 49,
  totalTokens: 99,
};
```

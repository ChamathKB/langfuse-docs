---
description: A decorator-based integration to give you powerful tracing, evals, and analytics for your LLM application
---

# Decorator-based Python Integration

Integrate [Langfuse Tracing](/docs/tracing) into your LLM applications with the Langfuse Python SDK using the `@observe()` decorator.

The SDK supports both synchronous and asynchronous functions, automatically handling traces, spans, and generations, along with key execution details like inputs and outputs and timings. This setup allows you to concentrate on developing high-quality applications while benefitting from observability insights with minimal code. The decorator is fully itneroperable with our main integrations (more on this below): [OpenAI](/docs/integrations/openai), [Langchain](/docs/integrations/langchain), [LlamaIndex](/docs/integrations/llama-index)

See the [reference](https://feat-add-tracing-decorators.langfuse-python.pages.dev/langfuse/decorators) for a comprehensive list of all available parameters and methods.

Want more control over the traces logged to Langfuse? Check out the [low-level Python SDK](/docs/sdk/python/low-level-sdk).

## Example

_Simple example (decorator + openai integration)_

```python /@observe()/ filename="main.py"
from langfuse.decorators import observe
from langfuse.openai import openai # OpenAI integration

@observe()
def story():
    return openai.chat.completions.create(
        model="gpt-3.5-turbo",
        max_tokens=100,
        messages=[
          {"role": "system", "content": "You are a great storyteller."},
          {"role": "user", "content": "Once upon a time in a galaxy far, far away..."}
        ],
    ).choices[0].message.content

@observe()
def main():
    return story()

main()
```

_Trace in Langfuse ([public link](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/fac231bc-90ee-490a-aa32-78c4269474e3?observation=36544d09-dec7-48ff-88c3-6c2ae3fe2baf))_

<Frame fullWidth>
  ![Simple OpenAI decorator
  trace](/images/docs/python-decorator-simple-trace.png)
</Frame>

## Installation & setup

<Steps>

### Install the Langfuse Python SDK

[![PyPI](https://img.shields.io/pypi/v/langfuse?style=flat-square)](https://pypi.org/project/langfuse/)

```bash
pip install langfuse
```

### Add Langfuse API keys

If you haven't done so yet, [sign up to Langfuse](https://cloud.langfuse.com/auth/sign-up) and obtain your API keys from the project settings. Alternatively, you can also run Langfuse locally or self-host.

import PythonEnv from "@/components-mdx/env-python.mdx";
import PythonEnvOs from "@/components-mdx/env-python-os.mdx";

<Tabs items={["os.environ", "python-dotenv"]}>
  <Tab>
    <PythonEnvOs />
  </Tab>
  <Tab>
    Use [`python-dotenv`](https://pypi.org/project/python-dotenv/) to load the
    environment variables from a `.env` file at the root of your application.
    <PythonEnv />
  </Tab>
</Tabs>

### Add the Langfuse decorator

Import the `@observe()` decorator and apply it to the functions you want to trace. By default it captures:

- nesting via context vars
- timings/durations
- args and kwargs as input dict
- returned values as output

The decorator will automatically create a trace for the top-level function and spans for any nested functions. Learn more about the tracing data model [here](/docs/tracing/overview).

```python /@observe()/
from langfuse.decorators import observe

@observe()
def fn():
    pass

@observe()
def main():
    fn()

main()
```

import { Callout } from "nextra/components";

<Callout type="info">
  Done! âœ¨ Read on to learn how to capture additional information, LLM calls,
  and more with Langfuse Python decorators.
</Callout>

<Callout type="warning">

In a short-lived environment like AWS Lambda, make sure to call `flush()` before the
function terminates to avoid losing events.

```python /langfuse_context.flush()/ /langfuse_context/
from langfuse.decorators import observe, langfuse_context

@observe()
def main():
    print("Hello, from the main function!")

main()

langfuse_context.flush()
```

</Callout>

</Steps>

## Decorator arguments

> See [SDK reference](https://feat-add-tracing-decorators.langfuse-python.pages.dev/langfuse/decorators#observe) for full details.

### Specify `as_type` of created observation

Observations in Langfuse can be of type `span`, `event`, and `generation` (learn more about the [tracing data model](/docs/tracing/overview)). By default, the `@observe()` decorator creates spans for nested functions and traces for the top-level function.

You can override this behavior by specifying the `as_type` parameter.

```python /@observe(as_type="generation")/
from langfuse.decorators import observe

@observe(as_type="generation")
def fn():
    pass

@observe()
def main():
    fn()

main()
```

### Disable capturing of input/output

By default, the `@observe()` decorator captures the input arguments and output results of the function. You can disable this behavior by setting the `capture_input` and `capture_output` parameters to `False`.

```python /capture_input=False/ /capture_output=False/
from langfuse.decorators import observe

@observe(capture_input=False, capture_output=False)
def fn(secret_arg):
    return "super secret output"

fn("my secret arg")
```

You can manually set the input and output of the observation using `langfuse_context.update_current_observation` (details below).

```python /langfuse_context.update_current_observation/ /capture_input=False/ /capture_output=False/
from langfuse.decorators import langfuse_context, observe

@observe(capture_input=False, capture_output=False)
def fn(secret_arg):
    langfuse_context.update_current_observation(
        input="sanitized input", # any serializable object
        output="sanitized output", # any serializable object
    )
    return "super secret output"

fn("my secret arg")
```

This will result in a trace with only sanitized input and output, and no actual function arguments or return values.

## Decorator context

Enhancing the detail and relevance of your observability data in Langfuse is straightforward. By leveraging the `langfuse_context.update_current_observation` and `langfuse_context.update_current_trace` methods, you can enrich the context of your observability data directly within the scope of the function being observed.

When adding parameters, consider the specific observation type that is in context. The [Python SDK API Reference](https://feat-add-tracing-decorators.langfuse-python.pages.dev/langfuse/decorators#LangfuseDecorator.update_current_observation) provides a comprehensive list of the parameters you can set per observation type. Trace parameters can be updated from any point within the nested function hierarchy.

Below is an example demonstrating how to enrich traces and observations with custom parameters:

```python
from langfuse.decorators import langfuse_context, observe


@observe(as_type="generation")
def deeply_nested_llm_call():
    # Enrich the current observation with a custom name, input, and output
    langfuse_context.update_current_observation(
        name="Deeply nested LLM call", input="Ping?", output="Pong!"
    )
    # Set the parent trace's name from within a nested observation
    langfuse_context.update_current_trace(
        name="Trace name set from deeply_nested_llm_call",
        session_id="1234",
        user_id="5678",
        tags=["tag1", "tag2"],
        public=True
    )


@observe()
def nested_span():
    # Update the current span with a custom name and level
    langfuse_context.update_current_observation(name="Nested Span", level="WARNING")
    deeply_nested_llm_call()


@observe()
def main():
    nested_span()


# Execute the main function to generate the enriched trace
main()

# Flush the enriched data to the Langfuse platform for analysis
langfuse_context.flush()
```

On the Langfuse platform the trace now shows with the updated name from the `deeply_nested_llm_call`, and the observations will be enriched with the appropriate data points.

![python_decorators_enriched-nesting](/images/cookbook/python_decorators_enriched-nesting.png)

### Flush observations

The Langfuse SDK executes network requests in the background on a separate thread for better performance of your application. This can lead to lost events in short lived environments such as AWS Lambda functions when the Python process is terminated before the SDK sent all events to our backend.

To avoid this, ensure that the `langfuse_context.flush()` method is called before termination. This method is waiting for all tasks to have completed, hence it is blocking.

## Additional features

### Scoring

[Scores](https://langfuse.com/docs/scores/overview) are used to evaluate single observations or entire traces. They can created manually via the Langfuse UI or via the SDKs.

| Parameter | Type   | Optional | Description                                                           |
| --------- | ------ | -------- | --------------------------------------------------------------------- |
| name      | string | no       | Identifier of the score.                                              |
| value     | number | no       | The value of the score. Can be any number, often standardized to 0..1 |
| comment   | string | yes      | Additional context/explanation of the score.                          |

You can attach a score to the current observation context by calling `langfuse_context.score_current_observation`. You can also score the entire trace from anywhere inside the nesting hierarchy by calling `langfuse_context.score_current_trace`:

```python
from langfuse.decorators import langfuse_context, observe


# This will create a new span under the trace
@observe()
def nested_span():
    langfuse_context.score_current_observation(
        name="feedback-on-span",
        value=1,
        comment="I like how personalized the response is",
    )

    langfuse_context.score_current_trace(
        name="feedback-on-trace",
        value=1,
        comment="I like how personalized the response is",
    )


# This will create a new trace
@observe()
def main():
    nested_span()


main()

# Flush the trace to send it to the Langfuse platform
langfuse_context.flush()
```

### Custom IDs

If you have your own unique ID representing an execution (messageId, traceId, correlationId), you can easily set those as trace or observation IDs for effective lookups in Langfuse. To set a custom ID for a trace or observation, simply pass the `langfuse_observation_id` as a keyword argument _within the traced function_. Requiring `langfuse_observation_id` to be set as a keyword argument (kwarg) here rather than as a static decorator argument enables ID assignment at runtime.

```python
from langfuse.decorators import langfuse_context, observe


@observe()
def process_user_request(user_id, request_data, **kwargs):
    # Function logic here
    pass


def main():
    user_id = "user123"
    request_data = {"action": "login"}

    # Custom ID for tracking
    custom_observation_id = f"{user_id}-{request_data['action']}"
    process_user_request(
        user_id=user_id,
        request_data=request_data,
        # Pass the custom observation ID to the function
        langfuse_observation_id=custom_observation_id,
    )


main()

# Flush the trace to send it to the Langfuse platform
langfuse_context.flush()
```

Alternatively you may also score a trace or observation from outside its context, since the `trace_id` or the `trace_id` in combination with the `observation_id` are sufficient to attach a score even from outside the function context. See the [Python SDK docs](https://python.reference.langfuse.com/langfuse/client#Langfuse.score) on the score method on the Langfuse client object.

```python
from langfuse import Langfuse
from langfuse.decorators import langfuse_context, observe

# Initialize the Langfuse client
langfuse_client = Langfuse()


# Create a new trace
@observe()
def main():
    trace_id = langfuse_context.get_current_trace_id()

    return "function_result", trace_id


# Flush the trace to send it to the Langfuse platform
langfuse_context.flush()

# Execute the main function to generate a trace
_, trace_id = main()

# Score the trace from outside the trace context
langfuse_client.score(
    trace_id=trace_id,
    name="user-explicit-feedback",
    value=1,
    comment="I like how personalized the response is"
)
```

### Debug mode

Enable debug mode to get verbose logs. Set the debug mode via the environment variable `LANGFUSE_DEBUG=True`.

### Authentication check

Use `langfuse_context.auth_check()` to verify that your host and API credentials are valid.

### Releases and versions

Track `releases` in Langfuse to relate traces in Langfuse with the versioning of your application. This can be done by setting the environment variable `LANGFUSE_RELEASE` or setting it as a trace parameter.

If no release is set, this defaults to [common system environment names](https://github.com/langfuse/langfuse-python/blob/main/langfuse/environment.py#L3).

## API reference

See the [Python SDK API reference](https://feat-add-tracing-decorators.langfuse-python.pages.dev/langfuse/decorators) for more details.

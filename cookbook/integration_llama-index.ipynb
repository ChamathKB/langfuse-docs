{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlceIPalN3QR"
      },
      "source": [
        "---\n",
        "description: Cookbook with examples of the Langfuse Integration for LlamaIndex (Python).\n",
        "category: Integrations\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqBspBzuRk9C"
      },
      "source": [
        "# Cookbook: LlamaIndex Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1oaA7XYGOfX"
      },
      "source": [
        "This is a cookbook with examples of the Langfuse Integration for LlamaIndex (Python).\n",
        "\n",
        "Follow the [integration guide](https://langfuse.com/docs/integrations/llama-index/get-started) to add this integration to your LlamaIndex project. Note that the integration does not support LlamaIndex.TS yet. If you are interested in an integration with LlamaIndex.TS, add your upvote/comments to this [issue](https://github.com/orgs/langfuse/discussions/1291)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbSpd5EiZouE"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNyU6IzCZouE"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse llama_index --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpE57ujJZouE"
      },
      "source": [
        "Initialize the Langfuse client with your API keys from the project settings in the Langfuse UI and add them to your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dEdF-668ZouF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# get keys for your project from https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-***\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-***\"\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # for EU data region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # for US data region\n",
        "\n",
        "# your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"***\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THwAx3s6S2hI"
      },
      "source": [
        "At the root of your LlamaIndex application, you need to register Langfuse's `LlamaIndexCallbackHandler` in the LlamaIndex `Settings.callback_manager`. There are two ways to configure the `LlamaIndexCallbackHandler`, (1) via environment variables or (2) via constructor arguments. Here we will use the former method. For the latter method refer to the [docs](https://langfuse.com/docs/integrations/llama-index/get-started)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxs4-JiHTgVA"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.callbacks import CallbackManager\n",
        "from langfuse.llama_index import LlamaIndexCallbackHandler\n",
        "\n",
        "langfuse_callback_handler = LlamaIndexCallbackHandler() # get langfuse's llamaindex callback handler\n",
        "CallbackManager([langfuse_callback_handler]) # instantiate callback manager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQMjOeNtUkBW"
      },
      "source": [
        "## Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRSAaZXauDYr"
      },
      "source": [
        "### SimpleQA\n",
        "\n",
        "Simple QA chat bot using an OpenAI model and a prompt template with LlamaIndex's `chat_engine`.\n",
        "\n",
        "todo: add screenshot (https://cloud.langfuse.com/project/clr4qu8qv0000yu4ja339x02u/traces/3f244ae8-dc26-4ecb-98b5-94a48535e76e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tEDFn-xIsZR5"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.chat_engine import SimpleChatEngine\n",
        "\n",
        "prompt = (\n",
        "    \"You are a helpful chat bot. Context information can be found below.\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Langfuse is an open source LLM engineering platform to help teams collaboratively debug, \"\n",
        "    \"analyze and iterate on their LLM Applications.\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Given the context information and no prior knowledge, \"\n",
        "    \"answer the question of the user\"\n",
        ")\n",
        "\n",
        "# Initialize the LLM\n",
        "langfuse_callback_handler = LlamaIndexCallbackHandler() # get langfuse's llamaindex callback handler\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", callback_manager=CallbackManager([langfuse_callback_handler]))\n",
        "\n",
        "chat_engine = SimpleChatEngine.from_defaults(\n",
        "    system_prompt=prompt,\n",
        "    llm=llm\n",
        ")\n",
        "response = chat_engine.chat(\n",
        "    \"What is Langfuse?\"\n",
        ")\n",
        "\n",
        "langfuse_callback_handler.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-id771_JXwqj"
      },
      "source": [
        "### RetrievalQA\n",
        "\n",
        "Simple RAG QA application using an OpenAI model and `VectorStoreIndex` to create an `OpenAIAgent` able to answer question using the context stored in the vector store.\n",
        "\n",
        "todo: add screenshot (https://cloud.langfuse.com/project/clr4qu8qv0000yu4ja339x02u/traces/cf60e4a4-9cd3-4193-99a1-f19355752b37)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sI35KCbBoY37"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "city_info = {\n",
        "    \"Toronto\": \"Toronto is the largest city in Canada and has a diverse population. It's known for its many green spaces.\",\n",
        "    \"Seattle\": \"Seattle, located in the Pacific Northwest, is surrounded by water, mountains, and evergreen forests.\",\n",
        "    \"Chicago\": \"Chicago is known for its bold architecture and has a skyline punctuated by skyscrapers.\",\n",
        "    \"Boston\": \"Boston is one of the oldest municipalities in the United States and is known for its rich history.\",\n",
        "    \"Houston\": \"Houston is the most populous city in Texas and is famous for its space research and energy industry.\"\n",
        "}\n",
        "\n",
        "city_docs = [Document(text=info, metadata={\"city\": city}) for city, info in city_info.items()]\n",
        "\n",
        "# Create an index from city_docs\n",
        "index = VectorStoreIndex.from_documents(city_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nywMW3_Tq4Rt"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.agent.openai import OpenAIAgent\n",
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "from langfuse.llama_index import LlamaIndexCallbackHandler\n",
        "\n",
        "# initialize the LLM\n",
        "langfuse_callback_handler = LlamaIndexCallbackHandler() # get langfuse's llamaindex callback handler\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", callback_manager=CallbackManager([langfuse_callback_handler]))\n",
        "\n",
        "# define a query engine tool for the index with metadata\n",
        "query_engine = index.as_query_engine(llm=llm)\n",
        "tool_metadata = ToolMetadata(name=\"CityInfoTool\", description=\"Tool for querying city information.\")\n",
        "query_engine_tool = QueryEngineTool(query_engine=query_engine, metadata=tool_metadata)\n",
        "\n",
        "agent = OpenAIAgent.from_tools([query_engine_tool], llm=llm, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "046SmErrrHGc"
      },
      "outputs": [],
      "source": [
        "# example query about a specific city\n",
        "query = \"What is known about the city of Boston?\"\n",
        "response = agent.chat(query)\n",
        "langfuse_callback_handler.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNvLWOAXj0XP"
      },
      "source": [
        "### Agent with tools\n",
        "\n",
        "Sample application using an two `FunctionTools`, one tool allowing to add number and one useless dummy tool, to create an `OpenAIAgent` able to answer queries by selecting the relevant tool.\n",
        "\n",
        "todo: add screenshot (https://cloud.langfuse.com/project/clr4qu8qv0000yu4ja339x02u/traces/4c70c620-1a0e-4257-88a6-3088f818597d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HWl1-x3vmeG6"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two integers and returns the result.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "\n",
        "def useless_tool() -> str:\n",
        "    \"\"\"A tool that returns a fixed string.\"\"\"\n",
        "    return \"This is a useless output.\"\n",
        "\n",
        "useless_tool = FunctionTool.from_defaults(fn=useless_tool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MderPLkFjtk0"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.agent.openai import OpenAIAgent\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "from langfuse.llama_index import LlamaIndexCallbackHandler\n",
        "\n",
        "# initialize the LLM\n",
        "langfuse_callback_handler = LlamaIndexCallbackHandler() # get langfuse's llamaindex callback handler\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", callback_manager=CallbackManager([langfuse_callback_handler]))\n",
        "\n",
        "# create an OpenAIAgent with custom tools\n",
        "agent = OpenAIAgent.from_tools([useless_tool, add_tool], llm=llm, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYXmX90Uj7Ps",
        "outputId": "c2a6b9df-4a22-430a-fae9-bb76e492a0b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: What is 5 + 2?\n",
            "=== Calling Function ===\n",
            "Calling function: add with args: {\"a\":5,\"b\":2}\n",
            "Got output: 7\n",
            "========================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\"What is 5 + 2?\", tool_choice=\"auto\")\n",
        "langfuse_callback_handler.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhBN67qX8xS"
      },
      "source": [
        "## Adding scores to traces\n",
        "To add [scores](/docs/scores) to traces created with the Langchain integration, access the traceId via `langfuse_handler.get_trace_id()`\n",
        "\n",
        "todo: add screenshot (https://cloud.langfuse.com/project/clr4qu8qv0000yu4ja339x02u/traces/4c70c620-1a0e-4257-88a6-3088f818597d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "PudCopwEPFgh"
      },
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "\n",
        "# Trace langchain run via the Langfuse CallbackHandler as shown above\n",
        "\n",
        "# get id of the last created trace\n",
        "trace_id = langfuse_callback_handler.get_trace_id()\n",
        "\n",
        "# add score, e.g. via the Python SDK\n",
        "langfuse = Langfuse()\n",
        "trace = langfuse.score(\n",
        "    trace_id=trace_id,\n",
        "    name=\"user-explicit-feedback\",\n",
        "    value=1,\n",
        "    comment=\"I like how personalized the response is\"\n",
        ")\n",
        "\n",
        "langfuse_callback_handler.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEWWS8PGo4A1"
      },
      "source": [
        "## Interoperability with Langfuse Python SDK\n",
        "\n",
        "In Langfuse, we can get a LlamaIndex callback handler by simply calling `langfuse_context.get_current_llama_index_handler()` in the context of a trace or span when using `decorators`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1zlFuIimJfT"
      },
      "source": [
        "### How it works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3sBgSeuMp5o4"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from langfuse.decorators import langfuse_context, observe\n",
        "\n",
        "# create a trace via Langfuse decorators and get a LlamaIndex Callback handler for it\n",
        "@observe() # automtically log function as a trace to Langfuse\n",
        "def main():\n",
        "    # update trace attributes (e.g, name, session_id, user_id)\n",
        "    langfuse_context.update_current_trace(\n",
        "        name=\"custom-trace\",\n",
        "        session_id=\"user-1234\",\n",
        "        user_id=\"session-1234\",\n",
        "    )\n",
        "    # get the langchain handler for the current trace\n",
        "    langfuse_context.get_current_llama_index_handler()\n",
        "\n",
        "    # use the handler to trace LlamaIndex runs ...\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRX2zFCOmwXH"
      },
      "source": [
        "### Example\n",
        "\n",
        "We will run the same `JSONalyzeQueryEngine`, a query able to perform statistical analysis on JSON Lists, ultiple times at different places within the hierarchy of a trace.\n",
        "\n",
        "```\n",
        "TRACE: json-data-analysis\n",
        "|\n",
        "|-- SPAN: age_analysis\n",
        "|\n",
        "|-- SPAN: occupation_analysis\n",
        "|   |\n",
        "|   |-- SPAN: all_occupations_query\n",
        "|   |\n",
        "|   |-- SPAN: most_common_occupation_query\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzLCbEm40pg6"
      },
      "outputs": [],
      "source": [
        "%pip install sqlite-utils --upgrade # library needed to run JSONalyzeQueryEngine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RLJqgaRxzkcF"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.callbacks import CallbackManager\n",
        "from langfuse.llama_index import LlamaIndexCallbackHandler\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.query_engine import JSONalyzeQueryEngine\n",
        "\n",
        "json_list = [\n",
        "    {\n",
        "        \"name\": \"John Doe\",\n",
        "        \"age\": 30,\n",
        "        \"occupation\": \"Software Engineer\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Jane Smith\",\n",
        "        \"age\": 28,\n",
        "        \"occupation\": \"Data Scientist\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Alice Johnson\",\n",
        "        \"age\": 35,\n",
        "        \"occupation\": \"Product Manager\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# initialize the LLM\n",
        "langfuse_callback_handler = LlamaIndexCallbackHandler() # get langfuse's llamaindex callback handler\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", callback_manager=CallbackManager([langfuse_callback_handler]))\n",
        "\n",
        "# initialize JSONalyze Query Engine\n",
        "json_stats_query_engine = JSONalyzeQueryEngine(list_of_dict=json_list, llm=llm, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8BhziWFxo5L"
      },
      "outputs": [],
      "source": [
        "from langfuse.decorators import langfuse_context, observe\n",
        "\n",
        "@observe() # automtically log function as a trace to Langfuse\n",
        "def main_analysis():\n",
        "    # set trace name, session_id, and user_id\n",
        "    langfuse_context.update_current_trace(\n",
        "        name=\"json-data-analysis\",\n",
        "        session_id=\"session-analysis\",\n",
        "        user_id=\"user-1\"\n",
        "    )\n",
        "    age_analysis()\n",
        "    occupation_analysis()\n",
        "\n",
        "@observe() # automtically log function as a span to Langfuse\n",
        "def age_analysis():\n",
        "    query_str = \"What is the average age of the individuals in the dataset?\"\n",
        "    response = json_stats_query_engine.query(query_str)\n",
        "    print(\"Age Analysis:\", response)\n",
        "\n",
        "@observe() # automtically log function as a span to Langfuse\n",
        "def occupation_analysis():\n",
        "    all_occupations_query()\n",
        "    most_common_occupation_query()\n",
        "\n",
        "@observe() # automtically log function as a sub-span to Langfuse\n",
        "def all_occupations_query():\n",
        "    query_str = \"What are the different occupations among the individuals?\"\n",
        "    response = json_stats_query_engine.query(query_str)\n",
        "    print(\"Occupation Analysis:\", response)\n",
        "\n",
        "@observe() # automtically log function as a sub-span to Langfuse\n",
        "def most_common_occupation_query():\n",
        "    query_str = \"What is the most common occupation among the individuals?\"\n",
        "    response = json_stats_query_engine.query(query_str)\n",
        "    print(\"Most Common Occupation Analysis:\", response)\n",
        "\n",
        "main_analysis()\n",
        "langfuse_context.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ3q8iMDGOfd"
      },
      "source": [
        "View it in Langfuse\n",
        "\n",
        "todo: update screenshot (https://cloud.langfuse.com/project/clr4qu8qv0000yu4ja339x02u/traces/ca1f0f94-04ef-4eab-b5a4-453af4718ee8)\n",
        "\n",
        "![Trace of Nested Langchain Runs in Langfuse](https://langfuse.com/images/docs/langchain_python_trace_interoperability.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlceIPalN3QR"
      },
      "source": [
        "---\n",
        "description: Cookbook with examples of the Langfuse Integration for LlamaIndex (Python).\n",
        "category: Integrations\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqBspBzuRk9C"
      },
      "source": [
        "# Cookbook: LlamaIndex Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1oaA7XYGOfX"
      },
      "source": [
        "This is a cookbook with examples of the Langfuse Integration for LlamaIndex (Python).\n",
        "\n",
        "Follow the [integration guide](https://langfuse.com/docs/integrations/llama-index/get-started) to add this integration to your LlamaIndex project. Note that the integration does not support LlamaIndex.TS yet. If you are interested in an integration with LlamaIndex.TS, add your upvote/comments to this [issue](https://github.com/orgs/langfuse/discussions/1291)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbSpd5EiZouE"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNyU6IzCZouE"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse llama_index llama-index-callbacks-langfuse --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpE57ujJZouE"
      },
      "source": [
        "Initialize the Langfuse client with your API keys from the project settings in the Langfuse UI and add them to your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dEdF-668ZouF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# get keys for your project from https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-***\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-***\"\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # for EU data region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # for US data region\n",
        "\n",
        "# your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"***\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THwAx3s6S2hI"
      },
      "source": [
        "At the root of your LlamaIndex application, you need to register Langfuse's `LlamaIndexCallbackHandler` in the LlamaIndex `Settings.callback_manager`. There are two ways to configure the `LlamaIndexCallbackHandler`, (1) via environment variables or (2) via constructor arguments. Here we will use the former method. For the latter method refer to the [docs](https://langfuse.com/docs/integrations/llama-index/get-started)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dxs4-JiHTgVA"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "from langfuse.llama_index import LlamaIndexCallbackHandler\n",
        "\n",
        "langfuse_callback_handler = LlamaIndexCallbackHandler() # get langfuse's llamaindex callback handler\n",
        "Settings.callback_manager = CallbackManager([langfuse_callback_handler]) # register callback handler in settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQMjOeNtUkBW"
      },
      "source": [
        "## Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRSAaZXauDYr"
      },
      "source": [
        "### SimpleQA\n",
        "\n",
        "Simple QA chat bot using an OpenAI model and a prompt template with LlamaIndex's `chat_engine`.\n",
        "\n",
        "todo: add screenshot (https://cloud.langfuse.com/project/clr4qu8qv0000yu4ja339x02u/traces/3f244ae8-dc26-4ecb-98b5-94a48535e76e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tEDFn-xIsZR5"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.chat_engine import SimpleChatEngine\n",
        "\n",
        "prompt = (\n",
        "    \"You are a helpful chat bot. Context information can be found below.\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Langfuse is an open source LLM engineering platform to help teams collaboratively debug, \"\n",
        "    \"analyze and iterate on their LLM Applications.\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Given the context information and no prior knowledge, \"\n",
        "    \"answer the question of the user\"\n",
        ")\n",
        "\n",
        "chat_engine = SimpleChatEngine.from_defaults(system_prompt=prompt)\n",
        "response = chat_engine.chat(\n",
        "    \"What is Langfuse?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-id771_JXwqj"
      },
      "source": [
        "### RetrievalQA\n",
        "\n",
        "Simple RAG QA application using an OpenAI model and `VectorStoreIndex` to create an `OpenAIAgent` able to answer question using the context stored in the vector store.\n",
        "\n",
        "todo: add screenshot (https://cloud.langfuse.com/project/clr4qu8qv0000yu4ja339x02u/traces/cf60e4a4-9cd3-4193-99a1-f19355752b37)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sI35KCbBoY37"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "city_info = {\n",
        "    \"Toronto\": \"Toronto is the largest city in Canada and has a diverse population. It's known for its many green spaces.\",\n",
        "    \"Seattle\": \"Seattle, located in the Pacific Northwest, is surrounded by water, mountains, and evergreen forests.\",\n",
        "    \"Chicago\": \"Chicago is known for its bold architecture and has a skyline punctuated by skyscrapers.\",\n",
        "    \"Boston\": \"Boston is one of the oldest municipalities in the United States and is known for its rich history.\",\n",
        "    \"Houston\": \"Houston is the most populous city in Texas and is famous for its space research and energy industry.\"\n",
        "}\n",
        "\n",
        "city_docs = [Document(text=info, metadata={\"city\": city}) for city, info in city_info.items()]\n",
        "\n",
        "# Create an index from city_docs\n",
        "index = VectorStoreIndex.from_documents(city_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nywMW3_Tq4Rt"
      },
      "outputs": [],
      "source": [
        "from llama_index.agent.openai import OpenAIAgent\n",
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "\n",
        "# define a query engine tool for the index with metadata\n",
        "query_engine = index.as_query_engine()\n",
        "tool_metadata = ToolMetadata(name=\"CityInfoTool\", description=\"Tool for querying city information.\")\n",
        "query_engine_tool = QueryEngineTool(query_engine=query_engine, metadata=tool_metadata)\n",
        "\n",
        "agent = OpenAIAgent.from_tools([query_engine_tool], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "046SmErrrHGc",
        "outputId": "99c830a2-1695-47f1-ec39-cfdfec6e58a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: What is known about the city of Boston?\n",
            "=== Calling Function ===\n",
            "Calling function: CityInfoTool with args: {\"input\":\"Boston\"}\n",
            "Got output: Boston is one of the oldest municipalities in the United States and is known for its rich history.\n",
            "========================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# example query about a specific city\n",
        "query = \"What is known about the city of Boston?\"\n",
        "response = agent.chat(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNvLWOAXj0XP"
      },
      "source": [
        "### Agent with tools\n",
        "\n",
        "Sample application using an two `FunctionTools`, one tool allowing to add number and one useless dummy tool, to create an `OpenAIAgent` able to answer queries by selecting the relevant tool.\n",
        "\n",
        "todo: add screenshot (https://cloud.langfuse.com/project/clr4qu8qv0000yu4ja339x02u/traces/4c70c620-1a0e-4257-88a6-3088f818597d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HWl1-x3vmeG6"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two integers and returns the result.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "\n",
        "def useless_tool() -> str:\n",
        "    \"\"\"A tool that returns a fixed string.\"\"\"\n",
        "    return \"This is a useless output.\"\n",
        "\n",
        "useless_tool = FunctionTool.from_defaults(fn=useless_tool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MderPLkFjtk0"
      },
      "outputs": [],
      "source": [
        "from llama_index.agent.openai import OpenAIAgent\n",
        "\n",
        "# create an OpenAIAgent with custom tools\n",
        "agent = OpenAIAgent.from_tools([useless_tool, add_tool], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYXmX90Uj7Ps",
        "outputId": "49bb1f07-cecb-4100-aeb9-e1621c937a01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: What is 5 + 2?\n",
            "=== Calling Function ===\n",
            "Calling function: add with args: {\"a\":5,\"b\":2}\n",
            "Got output: 7\n",
            "========================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\"What is 5 + 2?\", tool_choice=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhBN67qX8xS"
      },
      "source": [
        "## Adding scores to traces\n",
        "To add [scores](/docs/scores) to traces created with the Langchain integration, access the traceId via `langfuse_handler.get_trace_id()`\n",
        "\n",
        "todo: add screenshot (https://cloud.langfuse.com/project/clr4qu8qv0000yu4ja339x02u/traces/4c70c620-1a0e-4257-88a6-3088f818597d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PudCopwEPFgh"
      },
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "\n",
        "# Trace langchain run via the Langfuse CallbackHandler as shown above\n",
        "\n",
        "# get id of the last created trace\n",
        "trace_id = langfuse_callback_handler.get_trace_id()\n",
        "\n",
        "# add score, e.g. via the Python SDK\n",
        "langfuse = Langfuse()\n",
        "trace = langfuse.score(\n",
        "    trace_id=trace_id,\n",
        "    name=\"user-explicit-feedback\",\n",
        "    value=1,\n",
        "    comment=\"I like how personalized the response is\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEWWS8PGo4A1"
      },
      "source": [
        "## Interoperability with Langfuse Python SDK\n",
        "\n",
        "In Langfuse, we can get a LlamaIndex callback handler by simply calling `langfuse_context.get_current_llama_index_handler()` in the context of a trace or span when using `decorators`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1zlFuIimJfT"
      },
      "source": [
        "### How it works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3sBgSeuMp5o4"
      },
      "outputs": [],
      "source": [
        "from langfuse.decorators import langfuse_context, observe\n",
        "\n",
        "# create a trace via Langfuse decorators and get a LlamaIndex Callback handler for it\n",
        "@observe() # automtically log function as a trace to Langfuse\n",
        "def main():\n",
        "    # update trace attributes (e.g, name, session_id, user_id)\n",
        "    langfuse_context.update_current_trace(\n",
        "        name=\"custom-trace\",\n",
        "        session_id=\"user-1234\",\n",
        "        user_id=\"session-1234\",\n",
        "    )\n",
        "    # get the langchain handler for the current trace\n",
        "    langfuse_context.get_current_llama_index_handler()\n",
        "\n",
        "    # use the handler to trace LlamaIndex runs ...\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRX2zFCOmwXH"
      },
      "source": [
        "### Example\n",
        "\n",
        "We will run a `RetrievalQA` agent as seen before, to nest multiple two questions on a set of books in one trace.\n",
        "\n",
        "```\n",
        "TRACE: book-recommendation-analysis\n",
        "|\n",
        "|-- SPAN: genre_analysis\n",
        "|   |\n",
        "|   |-- SPAN: crete_genre_index\n",
        "|   |\n",
        "|   |-- SPAN: query_for_genre\n",
        "|\n",
        "|-- SPAN: author_analysis\n",
        "|   |\n",
        "|   |-- SPAN: crete_author_index\n",
        "|   |\n",
        "|   |-- SPAN: query_for_author\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PoEls0AoAmi"
      },
      "outputs": [],
      "source": [
        "from langfuse.decorators import langfuse_context, observe\n",
        "from llama_index.core import Settings, Document, VectorStoreIndex\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "from llama_index.agent.openai import OpenAIAgent\n",
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "\n",
        "# Sample book data\n",
        "book_data = {\n",
        "    \"genres\": {\n",
        "        \"Fantasy\": [\"Harry Potter and the Sorcerer's Stone\", \"The Hobbit\", \"A Game of Thrones\"],\n",
        "        \"Science Fiction\": [\"Dune\", \"The Martian\", \"Neuromancer\"],\n",
        "        \"Mystery\": [\"The Girl with the Dragon Tattoo\", \"Gone Girl\", \"The Da Vinci Code\"]\n",
        "    },\n",
        "    \"authors\": {\n",
        "        \"J.K. Rowling\": [\"Harry Potter and the Sorcerer's Stone\", \"Harry Potter and the Chamber of Secrets\"],\n",
        "        \"George R.R. Martin\": [\"A Game of Thrones\", \"A Clash of Kings\"],\n",
        "        \"Agatha Christie\": [\"Murder on the Orient Express\", \"And Then There Were None\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "@observe() # automtically log function as a trace to Langfuse\n",
        "def main_recommendation_system():\n",
        "    # Set callback manager for LlamaIndex, will apply to all LlamaIndex executions in this function\n",
        "    langfuse_handler = langfuse_context.get_current_llama_index_handler()\n",
        "    Settings.callback_manager = CallbackManager([langfuse_handler])\n",
        "\n",
        "    genre_analysis()\n",
        "    author_analysis()\n",
        "\n",
        "    langfuse_context.update_current_trace(\n",
        "        name=\"book-recommendation-analysis\",\n",
        "        session_id=\"session-bookrec\",\n",
        "        user_id=\"user-3\"\n",
        "    )\n",
        "\n",
        "@observe() # automtically log function as a span to Langfuse\n",
        "def genre_analysis():\n",
        "    # Set callback manager for LlamaIndex, will apply to all LlamaIndex executions in this function\n",
        "    langfuse_handler = langfuse_context.get_current_llama_index_handler()\n",
        "    Settings.callback_manager = CallbackManager([langfuse_handler])\n",
        "\n",
        "    genre_index = create_genre_index()\n",
        "    query_for_genre(genre_index, \"Fantasy\")\n",
        "\n",
        "@observe() # automtically log function as a sub-span to Langfuse\n",
        "def create_genre_index():\n",
        "    # Set callback manager for LlamaIndex, will apply to all LlamaIndex executions in this function\n",
        "    langfuse_handler = langfuse_context.get_current_llama_index_handler()\n",
        "    Settings.callback_manager = CallbackManager([langfuse_handler])\n",
        "\n",
        "    genre_docs = [Document(text=\", \".join(titles), metadata={\"genre\": genre})\n",
        "                  for genre, titles in book_data[\"genres\"].items()]\n",
        "    return VectorStoreIndex.from_documents(genre_docs)\n",
        "\n",
        "@observe() # automtically log function as a sub-span to Langfuse\n",
        "def query_for_genre(index, genre):\n",
        "    # Set callback manager for LlamaIndex, will apply to all LlamaIndex executions in this function\n",
        "    langfuse_handler = langfuse_context.get_current_llama_index_handler()\n",
        "    Settings.callback_manager = CallbackManager([langfuse_handler])\n",
        "\n",
        "    query_engine = index.as_query_engine()\n",
        "    tool_metadata = ToolMetadata(name=\"GenreTool\", description=\"Tool for querying book genres.\")\n",
        "    query_engine_tool = QueryEngineTool(query_engine=query_engine, metadata=tool_metadata)\n",
        "\n",
        "    agent = OpenAIAgent.from_tools([query_engine_tool], verbose=True)\n",
        "    query = f\"What books are in the {genre} genre?\"\n",
        "    response = agent.chat(query)\n",
        "\n",
        "@observe() # automtically log function as a span to Langfuse\n",
        "def author_analysis():\n",
        "    # Set callback manager for LlamaIndex, will apply to all LlamaIndex executions in this function\n",
        "    langfuse_handler = langfuse_context.get_current_llama_index_handler()\n",
        "    Settings.callback_manager = CallbackManager([langfuse_handler])\n",
        "\n",
        "    author_index = create_author_index()\n",
        "    query_for_author(author_index, \"J.K. Rowling\")\n",
        "\n",
        "@observe() # automtically log function as a sub-span to Langfuse\n",
        "def create_author_index():\n",
        "    # Set callback manager for LlamaIndex, will apply to all LlamaIndex executions in this function\n",
        "    langfuse_handler = langfuse_context.get_current_llama_index_handler()\n",
        "    Settings.callback_manager = CallbackManager([langfuse_handler])\n",
        "\n",
        "    author_docs = [Document(text=\", \".join(titles), metadata={\"author\": author})\n",
        "                   for author, titles in book_data[\"authors\"].items()]\n",
        "    return VectorStoreIndex.from_documents(author_docs)\n",
        "\n",
        "@observe() # automtically log function as a sub-span to Langfuse\n",
        "def query_for_author(index, author_name):\n",
        "    # Set callback manager for LlamaIndex, will apply to all LlamaIndex executions in this function\n",
        "    langfuse_handler = langfuse_context.get_current_llama_index_handler()\n",
        "    Settings.callback_manager = CallbackManager([langfuse_handler])\n",
        "\n",
        "    query_engine = index.as_query_engine()\n",
        "    tool_metadata = ToolMetadata(name=\"AuthorTool\", description=\"Tool for querying book authors.\")\n",
        "    query_engine_tool = QueryEngineTool(query_engine=query_engine, metadata=tool_metadata)\n",
        "\n",
        "    agent = OpenAIAgent.from_tools([query_engine_tool], verbose=True)\n",
        "    query = f\"What books has {author_name} written?\"\n",
        "    response = agent.chat(query)\n",
        "\n",
        "main_recommendation_system()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ3q8iMDGOfd"
      },
      "source": [
        "View it in Langfuse\n",
        "\n",
        "todo: update screenshot (https://cloud.langfuse.com/project/clr4qu8qv0000yu4ja339x02u/traces/225508bf-ec84-4c87-b881-0e539081bc5f)\n",
        "\n",
        "![Trace of Nested Langchain Runs in Langfuse](https://langfuse.com/images/docs/langchain_python_trace_interoperability.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

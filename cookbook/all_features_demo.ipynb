{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFad7S5hFuw3"
      },
      "source": [
        "# Langfuse feature demo using Langchain integration (Cookbook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbbjU5sqV8Fz"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9aD_hRSEGIL",
        "outputId": "0d17eddb-6754-453f-89dd-704505f55307"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langfuse in /usr/local/lib/python3.10/dist-packages (1.7.5)\n",
            "Requirement already satisfied: openai==0.28.1 in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: langchain==0.0.316 in /usr/local/lib/python3.10/dist-packages (0.0.316)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: npm in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (3.8.6)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (2.0.23)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (0.6.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (0.0.66)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (1.10.13)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (8.2.3)\n",
            "Requirement already satisfied: attrs>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from langfuse) (23.1.0)\n",
            "Requirement already satisfied: backoff<3.0.0,>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from langfuse) (2.2.1)\n",
            "Requirement already satisfied: httpx<0.26.0,>=0.15.4 in /usr/local/lib/python3.10/dist-packages (from langfuse) (0.25.1)\n",
            "Requirement already satisfied: monotonic<2.0,>=1.6 in /usr/local/lib/python3.10/dist-packages (from langfuse) (1.6)\n",
            "Requirement already satisfied: python-dateutil<3.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from langfuse) (2.8.2)\n",
            "Requirement already satisfied: pytz<2024.0,>=2023.3 in /usr/local/lib/python3.10/dist-packages (from langfuse) (2023.3.post1)\n",
            "Requirement already satisfied: wrapt==1.14 in /usr/local/lib/python3.10/dist-packages (from langfuse) (1.14.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: optional-django==0.1.0 in /usr/local/lib/python3.10/dist-packages (from npm) (0.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.316) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.316) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.316) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.316) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.316) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.26.0,>=0.15.4->langfuse) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<0.26.0,>=0.15.4->langfuse) (1.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.316) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.316) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0,>=2.8.0->langfuse) (1.16.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.316) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.316) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.316) (1.0.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<0.26.0,>=0.15.4->langfuse) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install langfuse openai==0.28.1 langchain==0.0.316 faiss-cpu tiktoken npm nltk --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zGlmnTcrGY5Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from operator import itemgetter\n",
        "\n",
        "import openai\n",
        "\n",
        "import langfuse.client\n",
        "from langfuse import Langfuse\n",
        "from langfuse.model import CreateTrace, CreateSpan, CreateGeneration, CreateEvent, CreateScore, CreateDatasetItemRequest, CreateDatasetRequest, InitialScore\n",
        "from langfuse.callback import CallbackHandler\n",
        "\n",
        "import langchain.schema\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda, base\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "from typing import Tuple\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7omt4IOFR3zm",
        "outputId": "7fa47125-c867-48a0-fbc7-6175bbe282d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download required NLTK data\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UsFjw3hgF0LP"
      },
      "outputs": [],
      "source": [
        "# Set open ai secret access key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"Insert your open AI secret access key here\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EeekdwGxYA6u"
      },
      "outputs": [],
      "source": [
        "  # Context database\n",
        "  professional_profiles = [\n",
        "      \"Harrison Fisher, 35 years old, based in Boston, formerly \"\n",
        "      \"worked in data analytics at Kensho. He now holds a role \"\n",
        "      \"in software development at AlphaTech.\",\n",
        "      \"Emily Becker, 29 years old, from San Francisco, was \"\n",
        "      \"previously a senior developer specializing in AI and \"\n",
        "      \"machine learning at Techwise. She is currently engaged \"\n",
        "      \"in advanced technology projects at AlphaTech.\",\n",
        "      \"Noah Schmidt, 40 years old, located in New York City, \"\n",
        "      \"previously worked in big data analytics at Quantum \"\n",
        "      \"Analytics. He is now focusing on blockchain technologies \"\n",
        "      \"in his position at AlphaTech.\",\n",
        "      \"Harrison Jones, 32 years old, residing in Seattle, \"\n",
        "      \"specialized in cloud computing and cybersecurity at \"\n",
        "      \"CloudTech Solutions. He currently contributes to \"\n",
        "      \"cybersecurity initiatives at AlphaTech.\",\n",
        "      \"Emily Fisher, 28 years old, based in Chicago, \"\n",
        "      \"previously a UI/UX designer at Creative Solutions. \"\n",
        "      \"She is now a lead designer for mobile applications \"\n",
        "      \"at AlphaTech.\",\n",
        "      \"Liam Fisher, 45 years old, from Austin, formerly a \"\n",
        "      \"project manager at Agile Dynamics. He is now \"\n",
        "      \"overseeing software development projects at AlphaTech.\",\n",
        "      \"Sophia Becker, 33 years old, living in Denver, was \"\n",
        "      \"a network engineer at NetCore. She's now in charge \"\n",
        "      \"of network infrastructure at AlphaTech.\",\n",
        "      \"Noah Becker, 30 years old, from Miami, former data \"\n",
        "      \"scientist at DataStream Analytics. He currently \"\n",
        "      \"works on AI-driven analytics at AlphaTech.\",\n",
        "      \"Mia Schmidt, 26 years old, based in Atlanta, was a \"\n",
        "      \"marketing strategist at BrandVision. She is now part \"\n",
        "      \"of the digital marketing team at AlphaTech.\",\n",
        "      \"Ethan Schmidt, 38 years old, residing in Philadelphia\"\n",
        "      \", previously a system administrator at TechNetworks. \"\n",
        "      \"Now focuses on IT security at AlphaTech.\",\n",
        "      \"Isabella Fisher, 31 years old, from Las Vegas, was a\"\n",
        "      \" database administrator at StorageTech. She's now \"\n",
        "      \"involved in big data projects at AlphaTech.\",\n",
        "      \"Mason Jones, 34 years old, living in Portland, \"\n",
        "      \"specialized in virtual reality at VR Innovations. \"\n",
        "      \"Currently leads the VR and AR development team at AlphaTech.\",\n",
        "      \"Charlotte Fisher, 27 years old, from Phoenix, former\"\n",
        "      \" software tester at Quality Assurance Inc. Now heads \"\n",
        "      \"the QA department at AlphaTech.\",\n",
        "      \"Logan Becker, 37 years old, based in Baltimore, was \"\n",
        "      \"a network security analyst at CyberSafe. He is now \"\n",
        "      \"part of the cybersecurity task force at AlphaTech.\",\n",
        "      \"Amelia Schmidt, 29 years old, residing in Minneapolis\"\n",
        "      \", formerly a graphic designer at Visual Arts Studio. \"\n",
        "      \"Currently works on user interface design at AlphaTech.\",\n",
        "      \"Jacob Fisher, 41 years old, from Dallas, former lead \"\n",
        "      \"developer at WebSolutions. Now oversees front-end \"\n",
        "      \"development at AlphaTech.\",\n",
        "      \"Zoe Schmidt, 36 years old, based in Orlando, previously\"\n",
        "      \" a business analyst at MarketGains. She now leads \"\n",
        "      \"the business intelligence team at AlphaTech.\",\n",
        "      \"Benjamin Becker, 39 years old, living in Sacramento,\"\n",
        "      \" specialized in software architecture at CodeCrafters.\"\n",
        "      \" He's now a senior software architect at AlphaTech.\",\n",
        "      \"Lily Jones, 30 years old, from Houston, was an IT \"\n",
        "      \"consultant at EfficientTech. Currently, she manages \"\n",
        "      \"cloud services and migration projects at AlphaTech.\",\n",
        "      \"William Becker, 33 years old, residing in San Diego,\"\n",
        "      \" former tech support manager at HelpDesk Pro. Now \"\n",
        "      \"leads the customer technology support team at AlphaTech.\"\n",
        "  ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QLuwoE3Y05n"
      },
      "source": [
        "## Langfuse Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdRHeT_WVN-I"
      },
      "source": [
        "Langfuse is an open source product analytics suite for LLM apps.\n",
        "\n",
        "Langfuse's goal is to enable product and engineering teams to harness the power of Generative AI with the most useful and open suite of devtools focused on visibility and insights.\n",
        "\n",
        "The figure below shows Langfuse's features along the development lifecycle. In this cookbook we are going to cover the features Traces ('Traces for debugging'), Scores ('User feedback collection' & 'Model-based evaluation'), Datasets ('Datasets for local testing'), and Analytics ('Monitor latency & cost')."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpLFGF9LVJqp"
      },
      "source": [
        "![Image of Langfuse feature along dev lifecycle](https://github.com/langfuse/langfuse-docs/tree/all_features_cookbook/public/images/docs/all_features_demo-cookbook-lifecycle.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj9Qhfa5p_qd"
      },
      "source": [
        "First, we need to set-up Langfuse\n",
        "1. Step: Open Langfuse user interface (UI) and sign in <br>\n",
        "2. Step: Create new project (in this cookbook the project is going to be called 'cookbook001') <br>\n",
        "3. Step: Create new API keys <br>\n",
        "4. Step: Copy API keys (secret API key will only be visible once) and set them as environmental variables in your environment <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTaPLXdPAmPu"
      },
      "source": [
        "Second, we authenticate ourself with our secret and public key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZIer8-rbADVp"
      },
      "outputs": [],
      "source": [
        "# Set Langfuse secret and public access keys\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"Insert your Langfuse secret access key here\"\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"Insert your Langfuse public access key here\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQtlLSxZY4ZZ"
      },
      "source": [
        "## Base model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_W1tSlJJHQy"
      },
      "source": [
        "Let us assume we were given an AI agent. In this example, the data scientist behind the model chose to build a retrieval augmented generative (RAG) architecture using the Langchain framework. The model is built as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SdOh23bRVT8k"
      },
      "outputs": [],
      "source": [
        "# Initialize retriever\n",
        "embedding = OpenAIEmbeddings(model='text-embedding-ada-002')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DiwjfZwhQ5Ky"
      },
      "outputs": [],
      "source": [
        "# Create vector database using similarity search algorithms in FAISS\n",
        "vectorstore = FAISS.from_texts(professional_profiles, embedding)\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NZ4yEYSrln99"
      },
      "outputs": [],
      "source": [
        "# Create prompt template\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7Dv_Ye07TJDh"
      },
      "outputs": [],
      "source": [
        "# Initialiize prompt template\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JNLMDMSbTRtG"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "s9qjpYH7tgfT"
      },
      "outputs": [],
      "source": [
        "# Initialize rag model\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3BDe8l4adRc"
      },
      "source": [
        "After putting the system into production, the integration has encountered significant issues. We have been receiving numerous complaints regarding the new system. The AI agent is reportedly delivering inaccurate information.\n",
        "\n",
        "Example: A notable example of these issues is highlighted in the inquiry about Harrison Jones' former employer. In this instance, the AI agent repeatedly provided incorrect data, exemplifying its propensity for errors in certain scenarios.\n",
        "\n",
        "Now we have been selected to fix the issue. We start to examine the error by invoking the agent with the reported error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DJgGagUqqHUd"
      },
      "outputs": [],
      "source": [
        "question = \"At which company was harrison jones working?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-5vV8T9mm-ue"
      },
      "outputs": [],
      "source": [
        "# Run chain with question\n",
        "answer = chain.invoke(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl5HGJsnp7xs",
        "outputId": "daf755cf-0405-4d73-cf82-48e66cd21253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input question: At which company was harrison jones working? \n",
            "Expected answer: CloudTech Solutions \n",
            "RAG answer: Harrison Jones was working at CloudTech Solutions.\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    f\"Input question: {question} \\n\"\n",
        "    f\"Expected answer: CloudTech Solutions \\n\"\n",
        "    f\"RAG answer: {answer}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVetBBsud52-"
      },
      "source": [
        "That is weird, right? <p>\n",
        "The AI agent seems to output the right answer ('CloudTech Solutions'). But users persist that the system is not working right. We would have to monitor what users are actually putting in and getting out of the system to see, understand, and solve the issue.\n",
        "\n",
        "But could we do to recreate and locate the error?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8zr3CI5WGlF"
      },
      "source": [
        "## Traces in Langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3JdLO8xeJcX"
      },
      "source": [
        "LLM apps use increasingly complex abstractions (chains, agents with tools, advanced prompts). The nested traces in Langfuse help to understand what is going on and get to the root cause of problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEjzHKV1fB4F"
      },
      "source": [
        "### First use of traces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffACatppK4ha"
      },
      "source": [
        "Let us start by creating a single trace to monitor one single call of the AI agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RewjJtajhepU"
      },
      "outputs": [],
      "source": [
        "# Initiate langfuse\n",
        "langfuse = Langfuse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NO7dAOD-g8Tl"
      },
      "outputs": [],
      "source": [
        "# Create new trace\n",
        "trace = langfuse.trace(CreateTrace(name=\"my_first_trace\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGFnmKIssMAc"
      },
      "source": [
        "Switching back to the UI, we can see that we have created a trace with the name 'my_first_trace'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0-Vu6jLdIVq"
      },
      "source": [
        "![Image of trace in UI](https://github.com/langfuse/langfuse-docs/tree/all_features_cookbook/public/images/docs/all_features_demo-cookbook-trace.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLXA2BlOL5D-"
      },
      "source": [
        "Let us now invoke the RAG architecture and trace the call using Langfuse's Langchain integration (since we used Langchain to build our system). We provide the handler of our trace as a callback to trace the prompt through the AI agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "t0GXTzSl_tlR"
      },
      "outputs": [],
      "source": [
        "# Get handler for trace\n",
        "handler = trace.get_langchain_handler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "G66Qp8IiFr8j"
      },
      "outputs": [],
      "source": [
        "# Run chain with question and provide handler as callback\n",
        "answer = chain.invoke(question, config={'callbacks':[handler]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mHSfilQf1ktv",
        "outputId": "ac14183b-6df8-460e-d92c-77df636acccd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://cloud.langfuse.com/trace/84f15412-fb6a-4c6c-aa6a-87bb67361b72'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The command get_trace_url() allows us to view and share our trace in the webbrowser\n",
        "trace.get_trace_url()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYKZgbepOIqL"
      },
      "source": [
        "If we follow the URL above (we could also see the trace through the UI) we can see the trace of the execution in our browser. In general, a trace gives us more details about the structure of our architecture and in- and output of each layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69nqe53bc8S0"
      },
      "source": [
        "![Image of trace overview in UI](https://github.com/langfuse/langfuse-docs/tree/all_features_cookbook/public/images/docs/all_features_demo-cookbook-trace_overview.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90igjLBpv-5t"
      },
      "source": [
        "We can see our trace consists of one observation (span 'RunnableSequence') which has 4 child observations (span 'RunnableParallel', span 'ChatPromptTemplate', generation 'ChatOpenAI', and span 'StrOutputParser') on its own. The input of the span 'RunnableSequence' is our input question (\"At which company was harrison jones working?\") and the output is the answer of the agent (\"Harrison Jones was working at CloudTech Solutions.\"). Let us have a quick look in each observation of the trace 'my_first_trace'. For more in-depth information on the function of each observation, please consult the [Langchain documentation](https://python.langchain.com/docs/get_started/introduction).\n",
        "\n",
        "1. RunnableSequence (child span of 'alphatech ems'): Describes the entire agent that was built. In Langchain a runnable sequence is a 'sequence of runnables, where the output of each is the input of the next' so in our case the chain we built\n",
        "2. RunnableParallel (child span of 'RunnableSequence'): Map of multiple runnables to run in parallel. In our case the input (question) and the context are mapped to match the input format of the prompt as defined in the prompt template. The question will just be passed through this span and the context will be retrieved from the FAISS vector database by the embedding. That is why this span has two child spans 'VectorStoreRetriever' and 'RunnablePassthrough'\n",
        "3. ChatPromptTemplate (child span of 'RunnableSequence'): Previously defined prompt template to provide instructions to the large language model\n",
        "4. ChatOpenAI (child generation of 'RunnableSequence'): OpenAI chat model ('gpt-3.5-turbo' by default) to answer question based on previous input and prompt template\n",
        "5. StrOutputParser (child span of 'RunnableSequence'): OutputParser to transform LLM output into string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErgWIFEonaN4"
      },
      "source": [
        "**Again, we see the agent seems to return the correct solution (\"Harrison Jones was working at CloudTech Solutions.\").**\n",
        "\n",
        "What now? We decide to implement langfuse completely into the application to monitor all the prompts made and be able to trace errors in real examples. To do so, we need to know about a few parameters of langfuse traces to enhance our system's analytics capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flp8gpW0eb_R"
      },
      "source": [
        "Traces can be used for more than just tracing calls of large language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzYpL5KOfHWU"
      },
      "source": [
        "### Features of traces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ec-qtagqq9Q"
      },
      "source": [
        "**1. Add releases & versions to traces**\n",
        "\n",
        "A release tracks the overall version of your application. Commonly it is set to the semantic version or git commit hash of your application.\n",
        "The version parameter can be added to traces and all observation types (span, generation, event). Thereby, you can track the effect of a new version on the metrics of an object with a specific name using.\n",
        "[More information here](https://langfuse.com/docs/experimentation)\n",
        "\n",
        "*Let us initiate langfuse with a release tag of 'v1.0'. In the next step, together with the user definition, we will create a trace with the version tag of our embedding model (\"text-embedding-ada-002\")*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XAi0CO91-w5A"
      },
      "outputs": [],
      "source": [
        "# Initiate langfuse with release tag 'v1.0'\n",
        "langfuse = Langfuse(release='v1.0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9QRzJYapvzy"
      },
      "source": [
        "**2. Add users to traces**\n",
        "\n",
        "Mapping data in Langfuse to individual users is easy by passing a unique identifier as the user_id attribute when creating a trace. This will also allow us to perform analytics on users in the UI. [More information here](https://langfuse.com/docs/user-explorer)\n",
        "\n",
        "*Let us create a test trace with a user_id*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Du4kyJSe-sKe"
      },
      "outputs": [],
      "source": [
        "# Create new trace with user_id ExampleUser1 and verion text-embedding-ada-002\n",
        "test_trace = langfuse.trace(CreateTrace(\n",
        "    name=\"my_second_trace\",\n",
        "    user_id=\"ExampleUser1\",\n",
        "    version=\"text-embedding-ada-002\"\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywpeGh9yv30-"
      },
      "source": [
        "**3. Add token usage to traces**\n",
        "\n",
        "When ingesting LLM generations into Langfuse you can add token usage numbers to the generation object. For OpenAI large language models the amount of prompt_tokens (input), completion_tokens (output), and total tokens (sum of input and output) is already retrieved through the OpenAI API call and saved in the trace. Some other models might not provide the token usage directly. In this case one might have to count tokens manually and provide them to the generation. [More information here](https://langfuse.com/docs/token-usage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDOP9kWgWsLc"
      },
      "source": [
        "### A first application of traces to log calls to an AI agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuWEvN-wEFeQ"
      },
      "source": [
        "In the following, we are going to write a few functions that will mimic the functioning of an AI agent with the integration of Langfuse. We first write a function to build the agent, then another one to create a trace and invoke the chain with the trace as a handler. Finally, the latter function returns the answer, the trace url and the trace_id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dHYsz148wLWk"
      },
      "outputs": [],
      "source": [
        "def build_agent() -> base.RunnableSequence:\n",
        "  \"\"\"\n",
        "  Build and return the chatbot agent.\n",
        "  The agent is initialized with a database of professional profiles,\n",
        "  uses a vector store for retrieving information, and answers questions\n",
        "  based on an OpenAI large language model enriched with a prompt\n",
        "  template.\n",
        "  \"\"\"\n",
        "  embedding = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
        "\n",
        "  # Create vector database using similarity search algorithms in FAISS\n",
        "  vectorstore = FAISS.from_texts(professional_profiles, embedding)\n",
        "  retriever = vectorstore.as_retriever()\n",
        "\n",
        "  # Create prompt template\n",
        "  template = \"\"\"Answer the question based only on the following context:\n",
        "  {context}\n",
        "\n",
        "  Question: {question}\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialiize prompt template\n",
        "  prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "  # Initialize model\n",
        "  model = ChatOpenAI()\n",
        "\n",
        "  # Initialize rag model\n",
        "  chain = (\n",
        "      {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "      | prompt\n",
        "      | model\n",
        "      | StrOutputParser()\n",
        "  )\n",
        "  return chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WaSksMT8wkZp"
      },
      "outputs": [],
      "source": [
        "def get_employee_information(\n",
        "        langfuse: Langfuse,\n",
        "        chain: base.RunnableSequence,\n",
        "        question: str,\n",
        "        userid: str = \"ExampleUser1\",\n",
        "        embedding_version: str = \"text-embedding-ada-002\"\n",
        ") -> Tuple[str, str, str, str]:\n",
        "    \"\"\"\n",
        "    Retrieves information based on a given question and\n",
        "    traces call with Langfuse's Langchain integration\n",
        "    \"\"\"\n",
        "    # Create a new trace with user ID and version\n",
        "    trace = langfuse.trace(CreateTrace(\n",
        "        name=\"agent_trace\",\n",
        "        user_id=userid,\n",
        "        version=embedding_version\n",
        "    ))\n",
        "    # Get the handler for the language chain\n",
        "    handler = trace.get_langchain_handler()\n",
        "    # Invoke the chain with the question and handler\n",
        "    answer = chain.invoke(question, config={'callbacks': [handler]})\n",
        "\n",
        "    return question, answer, trace.id, trace.get_trace_url()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2qRzxrMXf3t"
      },
      "source": [
        "Now we can test our application in a concrete example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85SjOvrYwwQk",
        "outputId": "bb0c044c-e653-450a-9a54-f4b01e2e107e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input question: Who from AlphaTech previously worked at BrandVision?\n",
            "Expected answer: 'Mia Schmidt'\n",
            "RAG answer: Mia Schmidt previously worked at BrandVision and is now part of the digital marketing team at AlphaTech.\n",
            "The trace 0f8a111b-5449-42e4-adfe-320d3de6229c is available here: https://cloud.langfuse.com/trace/0f8a111b-5449-42e4-adfe-320d3de6229c\n"
          ]
        }
      ],
      "source": [
        "# Initiate langfuse with release tag 'v1.0'\n",
        "langfuse = Langfuse(release='v1.0')\n",
        "# Initialize agent architecture\n",
        "chain = build_agent()\n",
        "# Define question\n",
        "question = \"Who from AlphaTech previously worked at BrandVision?\"\n",
        "# Run application (create trace, invoke agent, retrieve & trace answer)\n",
        "question, answer, id, url = get_employee_information(\n",
        "    langfuse,\n",
        "    chain,\n",
        "    question\n",
        "    )\n",
        "# Return question, answer, trace id, and trace url\n",
        "print(\n",
        "    f\"Input question: {question}\\n\"\n",
        "    f\"Expected answer: 'Mia Schmidt'\\n\"\n",
        "    f\"RAG answer: {answer}\\n\"\n",
        "    f\"The trace {id} is available here: {url}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVbGaCnNYLkj"
      },
      "source": [
        "## Scores in Langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oH4RzYVhYSr"
      },
      "source": [
        "Nice! Now we can finally have an idea of what our users are actually doing with our application but how do we find the problematic cases in which our chat bot fails to answer questions correctly?\n",
        "\n",
        "Would not it be nice to have a metric telling us how well our agent is doing? Luckily Langfuse got you covered by introducing scores for large language models. While there are an unlimited number of attributes a model can be evaluated on (Quality, Security, Style), there are 4 different ways to score a trace in Langfuse.\n",
        "\n",
        "1. Manuel scores: Review traces/generations and add scores manually in the UI <br>\n",
        "2. User-feedback: User evaluate the response to their prompt themselves (e.g., thumbs up/thumbs down) <br>\n",
        "3. Model-based: Using a model of how a human would use a proposed system to obtain predicted usability measures by calculation or simulation <br>\n",
        "4. Custom: Define a custom metric for your use-case to evaluate the model <br>\n",
        "[More information here](https://langfuse.com/docs/scores)\n",
        "\n",
        "We want to implement an user feedback option for users. The data science team already took care of implementing the new feature. Let us write an additional function that adds the user feedback to the trace if provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysj9ccIphYS4"
      },
      "source": [
        "Assumption: <br> Feedback is collected via positive/negative feedback (👍/:-1) with optional comments. Thus, the application gives back user_score and user_feedback where user_score is defined as:\n",
        "- User did not give feedback: user_score = 0, user_feedback = None\n",
        "- User did give 'thumbs up' feedback: user_score = 1, user_feedback = None/comment\n",
        "- User did give 'thumbs up' feedback: user_score = -1, user_feedback = None/comment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "h8Nwlc_thYS4"
      },
      "outputs": [],
      "source": [
        "def score_trace(\n",
        "        langfuse: Langfuse,\n",
        "        trace_id: str,\n",
        "        user_score: int,\n",
        "        user_feedback: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Adds a user score and comment to a trace.\n",
        "    \"\"\"\n",
        "    # Add score to trace with user's score and comment\n",
        "    langfuse.score(InitialScore(\n",
        "        name='User score',\n",
        "        traceId=trace_id,\n",
        "        value=user_score,\n",
        "        comment=user_feedback\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjbbrzSFhYS5"
      },
      "source": [
        "Test scoring function in application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8ooOmwNhYS5",
        "outputId": "f5b9a7ca-eb90-48aa-bd23-92f89c23d50e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input question: Where did Harrison work before joining Alphatech? \n",
            "Expected answer: 'Kensho' \n",
            "RAG answer: Harrison worked at CloudTech Solutions before joining AlphaTech. \n",
            "The trace 2fdfa17e-3dcd-4f7d-bab7-842a3b4ca0d9 is available here https://cloud.langfuse.com/trace/2fdfa17e-3dcd-4f7d-bab7-842a3b4ca0d9\n"
          ]
        }
      ],
      "source": [
        "# Initiate langfuse with release tag 'v1.0'\n",
        "langfuse = Langfuse(release='v1.0')\n",
        "\n",
        "# Initialize agent architecture\n",
        "chain = build_agent()\n",
        "\n",
        "# Define question\n",
        "question = \"Where did Harrison work before joining Alphatech?\"\n",
        "\n",
        "# Run application (create trace, invoke agent, retrieve & trace answer)\n",
        "question, answer, id, url = get_employee_information(\n",
        "    langfuse,\n",
        "    chain,\n",
        "    question)\n",
        "\n",
        "# Add user_score to trace\n",
        "score_trace(\n",
        "    langfuse,\n",
        "    id,\n",
        "    '-1',\n",
        "    'Wrong answer, Harrison Fisher worked at Kensho'\n",
        ")\n",
        "\n",
        "# Return question, answer, trace id, and trace url\n",
        "print(\n",
        "    f\"Input question: {question} \\n\"\n",
        "    f\"Expected answer: 'Kensho' \\n\"\n",
        "    f\"RAG answer: {answer} \\n\"\n",
        "    f\"The trace {id} is available here {url}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OBQg6zAhYS5"
      },
      "source": [
        "Thanks to this new feature we can now understand what is actually not working in our AI agent. Quickly, we find an example of an error. When asking for the employer of 'Harrison Fisher' prior to his job at Alphatech, the agent answered with 'Cloudtech Solutions' instead of 'Kensho'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4en7Bbujaer"
      },
      "source": [
        "![Image of score in UI](https://github.com/langfuse/langfuse-docs/tree/all_features_cookbook/public/images/docs/all_features_demo-cookbook-score.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eInWaYZ7qk2f"
      },
      "source": [
        "An examination of the trace of the call in langfuse exploits that the user asked for the employer of 'Harrison', so only stated the first name, not the family name. Looking at the VectorStoreRetriever you notice that there are two employees called 'Harrison' in the database, 'Harrison Jones' and 'Harrison Fisher'. The former was employed by 'Cloudtech Solution' which is why the agent returns this solution. To remediate this error we can now either change the prompt template to specify what the chat bot should answer if he does not get the full name or add more instruction into the context database for users."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GO-HmrTYtXk"
      },
      "source": [
        "## Datasets in Langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgEGxWpdoSm_"
      },
      "source": [
        "Now you have seen the power of Langfuse in a real use-case. But it does not stop here. Next, we ask ourselves how one can verify if changes one makes on a application, e.g., changing the prompt template, actually improve the performance of the application. That is were datasets become interesting.\n",
        "\n",
        "Datasets in Langfuse are a collection of inputs (and expected outputs) of an LLM application. They are used to benchmark new releases before deployment to production. Datasets can be incrementally created from new edge cases found in production.\n",
        "\n",
        "[More information here](https://langfuse.com/docs/datasets)\n",
        "\n",
        "Let us define a set of sample prompts to benchmark our agent and explore the impact of the change in the prompt template as mentioned before. To do so, we will also have to define a new score.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEWzcgtwVN5U"
      },
      "source": [
        "Note: Do not run the following section on datasets multiple times without changing the name of dataset. Otherwise the dataset will be extended by the question-answer in every run and thus, the computational costs will increase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "mAAdS8lcoOyr"
      },
      "outputs": [],
      "source": [
        "# Initialize dataset\n",
        "dataset = langfuse.create_dataset(CreateDatasetRequest(name=\"my_first_dataset\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "KVieB0AbukYL"
      },
      "outputs": [],
      "source": [
        "# Define dataset with question-answer pairs\n",
        "local_items = [\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"Where did Harrison Fisher work before joining AlphaTech?\"\n",
        "      },\n",
        "      \"expected_output\": \"Kensho\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"What was Emily Becker's role at her previous job?\"\n",
        "      },\n",
        "      \"expected_output\": \"Senior Developer, AI and Machine Learning\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"Which city is Noah Schmidt based in?\"\n",
        "      },\n",
        "      \"expected_output\": \"New York City\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"What is Harrison Jones's specialization?\"\n",
        "      },\n",
        "      \"expected_output\": \"Cloud Computing, Cybersecurity\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"What is Emily Fisher's current position at AlphaTech?\"\n",
        "      },\n",
        "      \"expected_output\": \"Lead Designer, Mobile Applications\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"What was Liam's previous role?\"\n",
        "      },\n",
        "      \"expected_output\": \"Project Manager\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"Where is Sophia currently working?\"\n",
        "      },\n",
        "      \"expected_output\": \"Network Infrastructure\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"What is Noah's expertise area at AlphaTech?\"\n",
        "      },\n",
        "      \"expected_output\": \"AI-Driven Analytics\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"What was Mia Schmidt's former job title?\"\n",
        "      },\n",
        "      \"expected_output\": \"Marketing Strategist\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"What is Ethan's focus area in his current role?\"\n",
        "      },\n",
        "      \"expected_output\": \"IT Security\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"Which city does Isabella Fisher reside in?\"\n",
        "      },\n",
        "      \"expected_output\": \"Las Vegas\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"What technology does Mason Jones specialize in at AlphaTech?\"\n",
        "      },\n",
        "      \"expected_output\": \"VR and AR Development\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"What department does Charlotte lead at AlphaTech?\"\n",
        "      },\n",
        "      \"expected_output\": \"QA Department\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"What was Logan Becker's specialization at his previous job?\"\n",
        "      },\n",
        "      \"expected_output\": \"Network Security Analyst\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"In which city does Amelia Schmidt live?\"\n",
        "      },\n",
        "      \"expected_output\": \"Minneapolis\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"What is Jacob Fisher's current role at AlphaTech?\"\n",
        "      },\n",
        "      \"expected_output\": \"Front-End Development Oversight\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"What team does Zoe Schmidt lead at AlphaTech?\"\n",
        "      },\n",
        "      \"expected_output\": \"Business Intelligence Team\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"What is Benjamin Becker's current position at AlphaTech?\"\n",
        "      },\n",
        "      \"expected_output\": \"Senior Software Architect\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"What is Lily Jones's role in cloud services at AlphaTech?\"\n",
        "      },\n",
        "      \"expected_output\": \"Cloud Services Management\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": {\n",
        "        \"question\": \"What was William's previous job before AlphaTech?\"\n",
        "      },\n",
        "      \"expected_output\": \"Tech Support Manager\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "IyloCu1Rv_ci"
      },
      "outputs": [],
      "source": [
        "# Upload dataset to langfuse\n",
        "for item in local_items:\n",
        "  langfuse.create_dataset_item(\n",
        "    CreateDatasetItemRequest(\n",
        "        dataset_name=\"my_first_dataset\",\n",
        "        input=item[\"input\"],\n",
        "        expected_output=item[\"expected_output\"]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN6xdubbwrbV"
      },
      "source": [
        "We can see in the UI that the dataset has been created and that it contains twenty items containing question-answer pairs to test our application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op2aSfoDk5zB"
      },
      "source": [
        "![Image of dataset in UI](https://github.com/langfuse/langfuse-docs/tree/all_features_cookbook/public/images/docs/all_features_demo-cookbook-dataset.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZQCpJneknNO"
      },
      "source": [
        "Now we want to run the dataset on our agent to measure its performance. Therefore, we first have to define a scoring method to evaluate the accuracy of our agent on a single invocation. After executing a dataset run we will be able to compute a score for the entire run (average of all item scores) and see the traces and scores for all items."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I23axKKZtN6f"
      },
      "source": [
        "To evaluate the agent's performance, we will use a custom scoring method focused on content rather than exact phrasing. This involves tokenizing and stemming both the expected and actual responses to standardize word forms. Then, we will calculate a fractional score based on the proportion of expected words found in the actual response, yielding a score between 0 (no match) and 1 (perfect match)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn97x5MLRuk5",
        "outputId": "58f69771-e540-43f4-9639-93218176fabf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score: 1.0\n"
          ]
        }
      ],
      "source": [
        "def tokenize_and_stem(text):\n",
        "    \"\"\"\n",
        "    Tokenize and stem the given text.\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return set(stemmed_tokens)\n",
        "\n",
        "def score_chatbot_response(response, expected_output):\n",
        "    \"\"\"\n",
        "    Score the chatbot's response against the expected output.\n",
        "    \"\"\"\n",
        "    # Tokenize and stem both response and expected output\n",
        "    response_tokens = tokenize_and_stem(response)\n",
        "    expected_tokens = tokenize_and_stem(expected_output)\n",
        "\n",
        "    # Check for token inclusion\n",
        "    matched_tokens = response_tokens.intersection(expected_tokens)\n",
        "    total_expected_tokens = len(expected_tokens)\n",
        "\n",
        "    # Calculate scoring\n",
        "    return len(matched_tokens) / total_expected_tokens if total_expected_tokens > 0 else 0.0\n",
        "\n",
        "\n",
        "# Example usage\n",
        "response = \"Harrison previously worked at Kensho before his tenure at AlphaTech.\"\n",
        "expected_output = \"Kensho\"\n",
        "score = score_chatbot_response(response, expected_output)\n",
        "print(f\"Score: {score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLQIFQmEvAUa"
      },
      "source": [
        "Next, we define a function to conduct a run, a sequential invocation of all items in a dataset. After each invocation, we add the score computed by our previously defined scoring function ```score_chat_response``` to the run item.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "mke5Bdw-0TdV"
      },
      "outputs": [],
      "source": [
        "def conduct_run(\n",
        "    langfuse: Langfuse,\n",
        "    chain: base.RunnableSequence,\n",
        "    experiment_name: str,\n",
        "    dataset_name: str = \"my_first_dataset\"\n",
        ") -> None:\n",
        "  dataset = langfuse.get_dataset(dataset_name)\n",
        "\n",
        "  for item in dataset.items:\n",
        "    # Get dataset item handler\n",
        "    handler = item.get_langchain_handler(run_name=experiment_name)\n",
        "    # Execute langchain chain\n",
        "    answer = chain.invoke(item.input[\"question\"], config={'callbacks':[handler]})\n",
        "    # Create score and add to rootSpan of run\n",
        "    handler.rootSpan.score(CreateScore(\n",
        "      name=\"check_keywords\",\n",
        "      value=score_chatbot_response(answer, item.expected_output)\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Hi4a5LUrbGNO"
      },
      "outputs": [],
      "source": [
        "# Conduct run\n",
        "conduct_run(langfuse, chain, \"first test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7sFV32mrgVZ"
      },
      "source": [
        "A quick look into the UI shows us that our run was successful. Although the average accuracy of our agent on the 20 test questions is only at 79% (see picture at the end of the chapter)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG45awWT2TQE"
      },
      "source": [
        "As suggested earlier, we are going to change the prompt template of the agent in the hope of improving our score. Let us ask the model to give back the answer to the question for all persons with the same name. This way we should be able to mitigate the errors similar to the one with 'Harrison Jones' and 'Harrison Fisher' we discovered earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "HeSwqlvg2Owg"
      },
      "outputs": [],
      "source": [
        "def build_improved_agent() -> base.RunnableSequence:\n",
        "  # Initialize retriever\n",
        "  embedding = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
        "\n",
        "  # Create vector database using similarity search algorithms in FAISS\n",
        "  vectorstore = FAISS.from_texts(professional_profiles, embedding)\n",
        "  retriever = vectorstore.as_retriever()\n",
        "\n",
        "  # Alter prompt template to answer questions to all persons with same name\n",
        "  template = \"\"\"Answer the question based only on the following context.\n",
        "  If the question contains only the first or the family name of a person,\n",
        "  please answer the question for all persons with this name.\n",
        "  Context:\n",
        "  {context}\n",
        "\n",
        "  Question: {question}\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialiize prompt template\n",
        "  prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "  # Initialize model\n",
        "  model = ChatOpenAI()\n",
        "\n",
        "  # Initialize rag model\n",
        "  chain = (\n",
        "      {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "      | prompt\n",
        "      | model\n",
        "      | StrOutputParser()\n",
        "  )\n",
        "  return chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "obHxMR-2r7j0"
      },
      "outputs": [],
      "source": [
        "# Build improved agent\n",
        "improved_chain = build_improved_agent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "17d5ob59r6M5"
      },
      "outputs": [],
      "source": [
        "# Conduct test run with improved agent\n",
        "conduct_run(langfuse, improved_chain, \"second test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auXOgAON3Ml7"
      },
      "source": [
        "But with a quick into the UI we discover that the average score on the question dataset is 79% again (see picture at the end of the chapter)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_IkoyBB4BH3"
      },
      "source": [
        "There has to be another issue with the agent. After some testing with the tracing functionality of langfuse, we find two different issues.\n",
        "1. The retriever only gives back 4 items from the dataset (default). In some cases the similarity search of the retriever is not able to retrieve the data samples of all persons with the same name\n",
        "2. The OpenAI large language model ```gpt-3.5-turbo```, which we are currently using, is performing far worse on large context prompts than ```gpt-4``` <br>\n",
        "\n",
        "Based on these findings we decide to increase the context size to 7 to find a trade-off between enough context to solve the answer and small costs due to limited number of input tokens. Also we change the model from ```gpt-3.5-turbo``` to  ```gpt-4```. Let's test our new agent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "svT6SjJTrZLD"
      },
      "outputs": [],
      "source": [
        "def build_final_agent() -> base.RunnableSequence:\n",
        "  # Initialize retriever\n",
        "  embedding = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
        "\n",
        "  # Create vector database using similarity search algorithms in FAISS\n",
        "  vectorstore = FAISS.from_texts(professional_profiles, embedding)\n",
        "  retriever = vectorstore.as_retriever(\n",
        "      search_type=\"mmr\",\n",
        "      search_kwargs={'k': 7}\n",
        "      )\n",
        "\n",
        "  # Create prompt template\n",
        "  template = \"\"\"Answer the question based only on the following context.\n",
        "  If the question contains only the first or the family name of a person,\n",
        "  please answer the question for all persons with this name.\n",
        "  Context:\n",
        "  {context}\n",
        "\n",
        "  Question: {question}\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialiize prompt template\n",
        "  prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "  # Initialize model\n",
        "  model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  # Initialize rag model\n",
        "  chain = (\n",
        "      {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "      | prompt\n",
        "      | model\n",
        "      | StrOutputParser()\n",
        "  )\n",
        "  return chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "OTWu8_SK5WZh"
      },
      "outputs": [],
      "source": [
        "# Build final agent\n",
        "final_chain = build_final_agent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "FyF_9J8A5WZp"
      },
      "outputs": [],
      "source": [
        "# Conduct test run with final agent\n",
        "conduct_run(langfuse, final_chain, \"third test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oNqskCEaDto"
      },
      "source": [
        "Now, we can see that our model is performing much better with a score 89%. One can go one and look into the traces that are not scoring '1.0' to further improve the agent although the remaining errors seem fairly individual at first glance (e.g., answer writes 'virtual reality' instead of 'vr' which is identical for our scoring function)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNNYsncNqFJr"
      },
      "source": [
        "![Image of dataset runs in UI](https://github.com/langfuse/langfuse-docs/tree/all_features_cookbook/public/images/docs/all_features_demo-cookbook-runs.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ5GVBkFatqF",
        "outputId": "f8ecf09b-f751-4f79-f6bc-7cb7bd71d7fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Harrison Jones worked at CloudTech Solutions before joining AlphaTech. Harrison Fisher worked at Kensho before joining AlphaTech.\n"
          ]
        }
      ],
      "source": [
        "print(final_chain.invoke(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH6ll4GYWMOj"
      },
      "source": [
        "Great! We see the chatbot answers the question correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbnqCIRhYDHy"
      },
      "source": [
        "## Analytics in Langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_3QlmN1drvg"
      },
      "source": [
        "Langfuse analytics derives actionable insights from production data. The analytics dashboard gives us an overview over the total number of traces, model costs, scores (empty by now), traces over  time, model usage over time (in costs or tokens used), user consumptions, latency per call, and more. Below is a snapshot of the dashboard after executing all the code above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Sk0-bPWs-dp"
      },
      "source": [
        "![Image of analytics in UI](https://github.com/langfuse/langfuse-docs/tree/all_features_cookbook/public/images/docs/all_features_demo-cookbook-analytics1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_yOTEUjt4O-"
      },
      "source": [
        "![Image of analytics in UI](https://github.com/langfuse/langfuse-docs/tree/all_features_cookbook/public/images/docs/all_features_demo-cookbook-analytics2.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
